{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1444fbae-0c4b-4896-aeb4-911c01009450",
   "metadata": {},
   "source": [
    "# Drift detection streaming pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d29bf-1ce0-461b-ab67-1bbb95319023",
   "metadata": {},
   "source": [
    "Diese Pipeline berechnet laufend den Datendrift der generierten Mushroom Daten. Sie dient als Illustration einer ganz einfachen Stream Processing Timeline mit Windowing Funktion.\n",
    "\n",
    "Für Anwendungsfälle, in denen es nicht zu erwarten ist, dass die Inferenzdaten sehr schnell driften, kann die Drift Detection natürlich auch als Batch Pipeline umgesetzt werden.\n",
    "\n",
    "Was fehlt:\n",
    " * Joins\n",
    " * Data Contracts\n",
    " * Event Notification design (Metadaten, sauberes Naming)\n",
    " * Event Notification format (better use avro with schema or suchlike)\n",
    " * Tests\n",
    " * Feature Engineering - Diese Pipeline testet den Drift der Rohdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bb3e0-4e3c-4c71-bd65-98a87927bdfe",
   "metadata": {},
   "source": [
    "Aufbau:\n",
    "* Der Datagen simuliert Prediction Request Events und schickt entsprechende Event Notifications an Kafka.\n",
    "* In diesen Nachrichten sind die Features eines Prediction requests (bzw. die Rohdaten, da wir in unserem vereinfachten Modell keine features berechnen) enthalten\n",
    "* Die Pipeline liest diese Notifications laufend von Kafka und sammelt alle Requests, bis eine akzeptable Menge zusammen hat\n",
    "* Sie vergleicht dann die aktuelle, von Kafka erhaltenen Daten mit dem Referenzdatenset, welches für das Training des Mushroom-Modells verwendet wurde, und berechnet pro Feature eine Kennzahl, welche den Drift angibt\n",
    "* Diese Kennzahlen senden sie an Statsd, wo sie von Prometheus gepollt werden, welcher die Daten als Zeitreihe speichert\n",
    "* Am Ende der Kette pollt Grafana diese Daten von Prometheus, um sie in einem Dashboard darstellen zu können, um bei zu grossem Drift Alarm zu schlagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3122acc-3ef6-4c59-a74e-368d26dda022",
   "metadata": {},
   "source": [
    "# Load reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7dd41-59a4-4115-ae09-bc18e68cdb83",
   "metadata": {},
   "source": [
    "Beachte, dass wir die Spalte mit dem Label gleich hier droppen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779840b3-5654-4df5-b542-3aafdfc84196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reference_df = pd.read_parquet('s3://traindata/train_raw.parquet', storage_options={\"anon\": False}).drop(\"class\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e798f6a-706d-4b18-8af1-e369a2eb04af",
   "metadata": {},
   "source": [
    "# Drift calculation and pushing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36d28e-2d31-450e-a69b-981289929d51",
   "metadata": {},
   "source": [
    "Funktionen, um den Drift pro Spalte zu berechnen und an statsd zu melden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbbcd1ac-1f34-4183-b469-84758ea37bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "from evidently import ColumnMapping\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def calculate_drift(df: pd.DataFrame, reference_df: pd.DataFrame,) -> tuple[tuple[str], tuple[float]]:\n",
    "\n",
    "    logger.info(f\"Received window of length {len(df)}\")\n",
    "    \n",
    "    # the dataframe generated from json has an index of dtype str, which we replace by an index of ints,\n",
    "    # or else evidently chokes\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # use a column mapping to make it easy for evidently to find the right metric\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.categorical_features = ['cap-shape', 'gill-attachment', 'gill-color', 'stem-color']\n",
    "    column_mapping.numerical_features = [c for c in df.columns if c not in column_mapping.categorical_features]\n",
    "\n",
    "    # define and execute evidently standard drift report\n",
    "    data_drift_dataset_report = Report(metrics=[DataDriftPreset()])\n",
    "    data_drift_dataset_report.run(reference_data=reference_df, current_data=df, column_mapping=column_mapping)\n",
    "\n",
    "    # extract a list of features and calculated drift metrics from report\n",
    "    report_whole_output = data_drift_dataset_report.as_dict()\n",
    "    report_just_drift = report_whole_output[\"metrics\"][1][\"result\"][\"drift_by_columns\"]\n",
    "    metrics_dict = {}\n",
    "    for column_name, column_dict in report_just_drift.items():\n",
    "        metrics_dict[column_name] = {k:v for k, v in column_dict.items() if k in ['stattest_name', 'drift_score']}\n",
    "        metrics_dict[column_name]['stattest_name'] = metrics_dict[column_name]['stattest_name'].replace(' ', '_')\n",
    "        metrics_dict[column_name]['drift_score'] = float(metrics_dict[column_name]['drift_score'])\n",
    "    features, metrics = zip(*metrics_dict.items())\n",
    "    \n",
    "    return features, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496d5ef5-adb2-4894-87b4-66cd41e922b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsd\n",
    "\n",
    "def report_drift_to_statsd(df: pd.DataFrame, reference_df: pd.DataFrame, statsd_client: statsd.client.udp.StatsClient) -> None:\n",
    "    \n",
    "    # calculate metric per column\n",
    "    features, metrics = calculate_drift(df, reference_df)\n",
    "\n",
    "    # push to statsd\n",
    "    prefix = f\"drift_metrics.mushroom.v1\"\n",
    "    for f, m in zip(features, metrics):\n",
    "        statsd_client.gauge(f\"{prefix}.{f}.{m['stattest_name']}\", m['drift_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b7b3c-c6d5-4a68-ac36-421886eb9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams import Application\n",
    "\n",
    "# create main quix object\n",
    "app = Application(\n",
    "    broker_address=\"message-broker:9092\",\n",
    "    # auto_offset_reset=\"earliest\"\n",
    ")\n",
    "\n",
    "# define topic and message format\n",
    "messages_topic = app.topic(name=\"mushroom_inference_request\", value_deserializer=\"json\")\n",
    "\n",
    "# create a StreamingDataFrame\n",
    "sdf = app.dataframe(topic=messages_topic)\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def initializer(value: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize the state for aggregation when a new window starts.\n",
    "\n",
    "    It will prime the aggregation when the first record arrives \n",
    "    in the window.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add a string index to the dict to get something like this\n",
    "    # we need this second level when combining multiple rows in\n",
    "    # the reducer, or else we just overwrite the same value\n",
    "    # again and again\n",
    "    \"\"\"\n",
    "    {0: {'cap-diameter': 3.0,\n",
    "      'cap-shape': 3.0,\n",
    "      'gill-attachment': 5.0,\n",
    "      'gill-color': 2.0,\n",
    "      'stem-height': 0.7591098099,\n",
    "      'stem-width': 1397.0,\n",
    "      'stem-color': 9.0,\n",
    "      'season': 0.9545582517}}\n",
    "    \"\"\"\n",
    "\n",
    "    return {str(k):v for k,v in enumerate([value])}\n",
    "\n",
    "\n",
    "def reducer(aggregated: dict, value: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Called on every row but the first\n",
    "\n",
    "    Reducer always receives two arguments:\n",
    "    - previously aggregated value (the \"aggregated\" argument)\n",
    "    - current value (the \"value\")\n",
    "    It combines them into a new aggregated value and returns it.\n",
    "    This aggregated value will be also returned as a q of the window.\n",
    "    \"\"\"\n",
    "\n",
    "    # first add old and new (without their respective index)\n",
    "    list_of_dicts = [value] + list(aggregated.values())\n",
    "    \n",
    "    # then readd an incremental index\n",
    "    return {str(k):v for k,v in enumerate(list_of_dicts)}\n",
    "\n",
    "\n",
    "statsd_client = statsd.StatsClient('statsd', 8125)\n",
    "\n",
    "sdf = (\n",
    "    # quix lacks the functionality to define a window of a fixed size, which would be appropriate here\n",
    "    # so instead as a crutch, we use a tumbling window, which works but is a bit weird\n",
    "    # don't make it too short, it should be long enough that reducer is called at least once\n",
    "    sdf.tumbling_window(duration_ms=timedelta(seconds=10))\n",
    "\n",
    "    # create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    # this is the quix way to define arbitrary aggregations (standard aggregations have their convenience functions)\n",
    "    .reduce(reducer=reducer, initializer=initializer)\n",
    "\n",
    "    # emit results only for closed windows\n",
    "    .final()\n",
    "\n",
    "    # now calculate drift statistics on full windows\n",
    "    .apply(lambda m: report_drift_to_statsd(pd.DataFrame(m[\"value\"]).T, reference_df, statsd_client))\n",
    ")\n",
    "\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d198cb0-db42-4224-8573-d5e17d9a1757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13aca2-68b8-4170-8896-8ff2d2038985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
