{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454dc9fb-c1b4-4a0b-b48c-21ad70a7507d",
   "metadata": {},
   "source": [
    "# Produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864fde8-aaaf-4708-903a-47c9eddbb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = \"message-broker:9092\"\n",
    ")\n",
    "\n",
    "topic = \"dev_sandbox\"\n",
    "\n",
    "def on_success(metadata):\n",
    "    print(f\"Message produced to topic '{metadata.topic}' at offset {metadata.offset}\")\n",
    "\n",
    "def on_error(e):\n",
    "    print(f\"Error sending message: {e}\")\n",
    "\n",
    "# Produce asynchronously with callbacks\n",
    "for i in range(1, 4):\n",
    "    msg = f\"Message with id #{i}\"\n",
    "    future = producer.send(topic, value=str.encode(msg))\n",
    "    future.add_callback(on_success)\n",
    "    future.add_errback(on_error)\n",
    "\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa691c-42d7-4a27-883c-249e053b38a9",
   "metadata": {},
   "source": [
    "# Consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05840ad1-b942-4787-8c2e-891271d4228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers=[\"message-broker:9092\"],\n",
    "    group_id=\"sandbox-group\",\n",
    "    # with the following settings, reads whole queue every time\n",
    "    # auto_offset_reset=\"earliest\",\n",
    "    # enable_auto_commit=False,\n",
    ")\n",
    "\n",
    "consumer.subscribe(\"dev_sandbox\")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        topic_info = f\"topic: {message.partition}|{message.offset}\"\n",
    "        message_info = f\"key: {message.key}, {message.value}\"\n",
    "        print(f\"{topic_info}, {message_info}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while consuming messages: {e}\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87f4af-86c2-4fef-b879-790fdf1cb4e5",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ff2a6-2b28-4174-9a80-49dfa41fde66",
   "metadata": {},
   "source": [
    "See also mushroom_datagen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6801f22-2ea2-4cf8-93d3-0f5c97bff34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "from loguru import logger\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = \"message-broker:9092\",\n",
    ")\n",
    "\n",
    "def on_success(metadata):\n",
    "    logger.debug(f\"Message produced to topic '{metadata.topic}' at offset {metadata.offset}\")\n",
    "\n",
    "\n",
    "def on_error(e):\n",
    "    logger.error(f\"Error sending message: {e}\")\n",
    "\n",
    "\n",
    "def setup_data():\n",
    "    # fetch Training Dataset so we have a reference\n",
    "    df = pd.read_parquet('s3://traindata/train_raw.parquet',\n",
    "                         storage_options={\"anon\": False}).drop(\"class\", axis=\"columns\")\n",
    "\n",
    "    # setup column names\n",
    "    categoricals = ['cap-shape', 'gill-attachment', 'gill-color', 'stem-color']\n",
    "    numericals = [c for c in df.columns if c not in categoricals]\n",
    "\n",
    "    # fit an estimator the the numerical columns\n",
    "    kde = KernelDensity()\n",
    "    kde.fit(df[numericals])\n",
    "\n",
    "    return kde, numericals, categoricals, df\n",
    "\n",
    "\n",
    "def generate_event(kde, numericals, categoricals, df):\n",
    "    # take one row so we have something to fill in our generated values\n",
    "    new_row = pd.DataFrame(data=df.head(1))\n",
    "\n",
    "    # generate one row\n",
    "    new_row[numericals] = kde.sample(1)\n",
    "    for col in numericals:\n",
    "        # we are being lazy, when the kde yields a negative, just replace with mean\n",
    "        if new_row[col][0] < 0:\n",
    "            new_row[col] = df[col].mean()\n",
    "    for col in categoricals:\n",
    "        # for the categoricals, use a bounded random value\n",
    "        new_row[col] = randrange(df[col].min(), df[col].max()+1)\n",
    "    for col in new_row.columns:\n",
    "        # make sure the datatypes match the reference\n",
    "        new_row[col] = new_row[col].astype(df[col].dtype)\n",
    "\n",
    "    # add drift\n",
    "    pass\n",
    "\n",
    "    return new_row.iloc[0].to_json()\n",
    "\n",
    "\n",
    "def push_to_kafka(event, topic):\n",
    "    # send to redpanda\n",
    "    future = producer.send(topic=topic, key=b'key', value=event.encode('utf-8'))\n",
    "    future.add_callback(on_success)\n",
    "    future.add_errback(on_error)\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def run(topic, sleep_interval, burst_size, randomness):\n",
    "    kde, numericals, categoricals, df = setup_data()\n",
    "    while True:\n",
    "        for _ in range(burst_size):\n",
    "            push_to_kafka(event=generate_event(kde, numericals, categoricals, df), topic=topic)\n",
    "\n",
    "        try:\n",
    "            sleep_milliseconds = sleep_interval + randrange(0, randomness*1000)\n",
    "        except ValueError:\n",
    "            # if randomness*1000 < 1\n",
    "            sleep_milliseconds = sleep_interval\n",
    "        time.sleep(sleep_milliseconds / 1000)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Simulates inference requests by sending mushroom dataset features to kafka.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-t\",\n",
    "        \"--topic\",\n",
    "        type=str,\n",
    "        help=\"Kafka topic to send to\",\n",
    "        default=\"mushroom_inference_request\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--sleep_interval\",\n",
    "        type=int,\n",
    "        help=\"Number of milliseconds to sleep between bursts\",\n",
    "        default=1000,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\",\n",
    "        \"--burst_size\",\n",
    "        type=int,\n",
    "        help=\"Number of messages to send together\",\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-r\",\n",
    "        \"--randomness\",\n",
    "        type=int,\n",
    "        help=\"Maximum number of seconds to wait between bursts\",\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-v\",\n",
    "        \"--verbose\",\n",
    "        action='store_true',\n",
    "        help=\"Log content of each message sent sto stderr\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #  todo drift toggle\n",
    "\n",
    "    if not args.verbose:\n",
    "        # increase loglevel from the default DEBUG to INFO to avoid logging every message\n",
    "        logger.remove(0)\n",
    "        logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "    logger.info(\"Start sending messages to kafka.\")\n",
    "    run(\n",
    "        topic = args.topic,\n",
    "        sleep_interval = args.sleep_interval,\n",
    "        burst_size = args.burst_size,\n",
    "        randomness = args.randomness\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
