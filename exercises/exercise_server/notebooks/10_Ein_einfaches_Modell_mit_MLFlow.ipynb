{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f171957",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Achtung:</b> Der hier angezeigt Code kann nicht direkt im Exercise Server ausgeführt werden. Der Code fuktioniert nur im <a href=\"http://localhost:8080/lab\">Jupyter Lab der Development Environment</a>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4694e0a",
   "metadata": {},
   "source": [
    "# Übersicht - Experiment Tracking und Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a9cfb",
   "metadata": {},
   "source": [
    "In dieser Übung erweitern wir den Code, welcher unser einfaches Modell trainiert, um Experiment Tracking. Wir führen einige Experimente durch und vergleichen deren Resultate in MLFlow. Wir fügen unseren Modellen Metadaten hinzu, registrieren ein Modell in der Registry und probieren kurz den MLFlow Client (das low-level API von MLFlow aus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b64ad5",
   "metadata": {},
   "source": [
    "# Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c4dba",
   "metadata": {},
   "source": [
    "## Notebook für den Code erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a5170",
   "metadata": {},
   "source": [
    "Kopiere als erstes dein vorheriges Notebook (`01-Simple_Model.ipynb`) mit dem Code, welcher das einfache ML Modell trainiert und gib der Kopie den Namen `02-Simple_Model_MLFlow.ipynb` und schliesse das nicht mehr verwendete Notebook `01-Simple_Model.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45902721",
   "metadata": {},
   "source": [
    "# Übungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf46b29",
   "metadata": {},
   "source": [
    "## MLFlow Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abe471",
   "metadata": {},
   "source": [
    "Importiere mlflow an einer geeigneten Stelle in deinem `02-Simple_Model_MLFlow.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4849220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466caf5",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    " Die Tracking URL brauchst Du nicht zu setzen. Erinnerst Du Dich, weshalb dies nicht notwendig ist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892671ee",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Die URL wird im docker-compose der Development Environment als Umgebungsvariable gesetzt:\n",
    "\n",
    "    environment:\n",
    "       - MLFLOW_TRACKING_URI=http://model-registry:5001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011577d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed802f4",
   "metadata": {},
   "source": [
    "Führe nun die Zellen vom Laden der Daten bis und mit Vorbereitung des finalen Testsets (`train_test_split()`) aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c065267",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Du musst dein MLFlow Experiment benennen. Das Experiment bündelt alle Versuche mit dem Mushroom Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2a55e",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Mushroom Categorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bc0fe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4554797",
   "metadata": {},
   "source": [
    "## Hyper-Parameter definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a62c3",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Nun baust Du den bestehenden Code so um, dass die Hyper-Parameter des Classifiers vor dessen Instanzierung in ein Dictionary gespeichert werden, und dann bei der Instanzierung des Classifiers dieser Dictionary übergeben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550b320",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# wenn wir mit der Musterlösung aus der vorherigen Übung weitermachen, könnte das zum Beispiel so aussehen:\n",
    "\n",
    "params = {\n",
    "    'kneighborsclassifier__n_neighbors': 3\n",
    "}\n",
    "pipeline = make_pipeline(preprocessor, clf)\n",
    "pipeline.set_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c6250",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc3ff0",
   "metadata": {},
   "source": [
    "## Training und Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8b994",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Jetzt geht es ans Evaluieren des Modells. Dazu müssen wir festlegen, welche Metriken wir verwenden (eigentlich ist dies ein sehr zentraler Schritt, welcher schon ganz am Anfang eines ML Projektes durchgeführt werden muss, denn die Metriken, mit denen wir das Modell evaluieren, müssen auf den Use Case und die für das Business relevanten Kennzahlen möglichst gut abgestimmt sein).\n",
    "\n",
    "Wir können hier die klassische Accuracy, Precision, Recall sowie F1-Score loggen. Importiere diese Metriken von `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa5d74",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa85cdb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3dfc1",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Wir validieren mit einem einfachen, fixen Validation Set. Verwende dazu `train_test_split()` noch einmal auf dem Trainingsset, um von diesem noch ein fixes Validierungsset abzuspalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6fab7",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "X_train_small, X_val, y_train_small, y_val = train_test_split(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4236d57",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70ffc6",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Damit können wir nun mit den `small` Varianten trainieren und mit `val`validieren. Bis und mit diesem Schritt gibt es keinen MLFLow-spezifischen Code.\n",
    "\n",
    "Verwende `fit()` und `y_pred = predict()`mit X_val, um zu trainieren und predictions für die nachfolgende Validierung zu machen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8816fe",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train_small, y_train_small)\n",
    "y_pred = pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bbce10",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67832ea",
   "metadata": {},
   "source": [
    "Nun füllen wir einen Dictionary mit unseren vier Metriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8aa7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_static_val_set = {\n",
    "    \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "    \"precision\": precision_score(y_val, y_pred),\n",
    "    \"recall\": recall_score(y_val, y_pred),\n",
    "    \"f1\": f1_score(y_val, y_pred), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4587507",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Und erstelle eine MLFlow Signatur für die Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909f93c",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "signature = infer_signature(model_input=X_train, model_output=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72135109",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a47975",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Und jetzt, wo wir *Signatur*, *Parameter*, *Modell* und *Scores* haben, können wir den MLFlow run starten und alle vier Dinge loggen.\n",
    "\n",
    "Als Erinnerung: Um zu vermeiden, dass bei einer Exception während des Trainings ein halb-fertiger run geloggt wird, starten wir den MLFlow run zum Loggen erst, wenn wir alle Komponenten zum Loggen beisammen haben, also erst nach Training und Evaluation.\n",
    "\n",
    "Gib dem run den Namen. Welchen Namen Du dem run gibst, ist abhängig davon, was Du gerade ausprobierst, welche Hyperparameter Du testen möchstest usw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47de727",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"baseline model\") as run:\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, signature=signature, artifact_path=\"mushroom\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12a41f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07a0c9",
   "metadata": {},
   "source": [
    "Wenn alles geklappt hat, sehen wir im [MLFLow UI](http://localhost:5001/), dass Metadaten, die vier Metriken, der oder die Parameter und das Modell geloggt wurden. Wenn wir auf eine Metrik klicken, wird diese als Plot angezeigt. Die Y-Achse entspricht hier dem Wert der Metrik, die X-Achse sogenanntgen *Steps*. Steps können verwendet werden, um ein Training über die Zeit abzubilden, also eine Learning Curve über Epochen, über Trainingssetgrössen oder andere Hyperparameter. Wenn wir in `log_metrics()` das `step=` Argument nicht angeben, wird standardmässig nur ein Step geloggt, was dann zum konstanten Plot führt, wie wir ihn hier sehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049d55c",
   "metadata": {},
   "source": [
    "## Runs vergleichen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da88121",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Ändere nun einen Parameter deines Modells und logge einen zweiten run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db23bb",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# code ausser die zweite Zeile und der run_name gleich wie bisher\n",
    "\n",
    "params = {\n",
    "    'kneighborsclassifier__n_neighbors': 8\n",
    "}\n",
    "pipeline.set_params(**params)\n",
    "\n",
    "pipeline.fit(X_train_small, y_train_small)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "metrics_static_val_set = {\n",
    "    \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "    \"precision\": precision_score(y_val, y_pred),\n",
    "    \"recall\": recall_score(y_val, y_pred),\n",
    "    \"f1\": f1_score(y_val, y_pred), \n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"another_run\") as run:\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics_static_val_set)\n",
    "    mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, signature=signature, artifact_path=\"mushroom\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf352f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12ca06",
   "metadata": {},
   "source": [
    "Du kannst nun im [MLFLow UI](http://localhost:5001/) beide runs selektieren und dann vergleichen. Du kannst verschiedene Plots anzeigen und die Unterschiede von Parametern und Metriken anzeigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcf56b",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8213c",
   "metadata": {},
   "source": [
    "Evaluation auf einem fixen Validation Set ist nur zu empfehlen, wenn wir viel Daten haben, und unser Validation Set gross genug und damit repräsentativ für das gesamte Trainingsset ist. Kleineren Datensets und schnell trainierende Modelle erlauben eine K-fold Cross Validation und dadurch präzisere Aussagen über unsere Modell-Performance. Wie würden wir Resultate einer K-fold Cross Validation in MLFlow loggen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac96234",
   "metadata": {},
   "source": [
    "Leider unterstützt MLFlow dies nicht direkt. Als Workaround könnten wir den Durchschnitt über unserer Folds loggen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a0a62",
   "metadata": {},
   "source": [
    "## Autolog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15291e9",
   "metadata": {},
   "source": [
    "Nun vereinfachen wir das Logging etwas, indem wir die autolog Funktion von MLFlow verwenden. Hierbei ist wichtig, dass die Autologging-Funktionalität aktiviert wird, *bevor* die verwendete Metrik von sklearn importiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd763856",
   "metadata": {},
   "source": [
    "**Starte deshalb den Jupyter Kernel neu (im Menu Kernel -> Restart).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad91e5c",
   "metadata": {},
   "source": [
    "Führe alle notwendigen Zellen bis zum Import von MLFlow aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bec30c",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Aktiviere die Autolog Funktion gleich unterhalb der Zelle, wo du das Experiment setzt, und somit *bevor* du die Metriken von sklearn importierst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101faee",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Mushroom Categorization\")\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55791b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc5aa8",
   "metadata": {},
   "source": [
    "Führe die restlichen Zellen aus bis und mit dem `fit()` deines Modells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e36837",
   "metadata": {},
   "source": [
    "Es wird autromatisch ein neuer Run geloggt, welchen du im [MLFLow UI](http://localhost:5001/) anschauen kannst. Diese Funktionalität ist sehr praktisch, da viele weitere Metadaten mitgeloggt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432798e",
   "metadata": {},
   "source": [
    "Aber... bis jetzt wurden erst die Trainings-Metriken geloggt, was natürlich nur bedingt nützlich ist. Um auch die Validierungsmetriken zu loggen, müssen die scorer ausgeführt werden, dies wird dann von autlog gecaptured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "metrics_static_val_set = {\n",
    "    \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "    \"precision\": precision_score(y_val, y_pred),\n",
    "    \"recall\": recall_score(y_val, y_pred),\n",
    "    \"f1\": f1_score(y_val, y_pred), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64161ac2",
   "metadata": {},
   "source": [
    "## Metadaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffe3c4",
   "metadata": {},
   "source": [
    "Runs können mit einer *Description* und mit *Tags* versehen werden. Dies kann entweder gleich während des Loggens des Runs passieren, oder nachträglich im UI oder mittels dem API (weiter unten beschrieben)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f919f",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Füge mittels UI einem Run eine Beschreibung und ein neues Tag hinzu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e779f",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Dies kann in der Overview Tab eines Runs gemacht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35570f1a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb96f8",
   "metadata": {},
   "source": [
    "Auch dem Modell selber kann man Metadaten hinzufügen. Diese erscheinen dann im `MLmodel` File."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a872aaa",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Logge einen neuen Run und füge dabei Model Metadaten hinzu. Suche diese Metadaten im UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b44e7",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# dem log_model muss ein metadata Parameter mit einem dictionary hinzugefügt werden\n",
    "# sichtgbar sind die Metadaten dann im Artifacts Tab im MLmodel File\n",
    "mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline, signature=signature, artifact_path=\"mushroom\", metadata={'foobar':'baz'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273dbc8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c436e14",
   "metadata": {},
   "source": [
    "## Models registrieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88fdd9",
   "metadata": {},
   "source": [
    "Bis jetzt haben wir uns noch rein in der Entwicklungsphase bewegt. Wenn wir einmal zum Punkt gekommen sind, an dem wir ein Modell haben, welches wir deployen möchten, müssen wir dieses *registrieren*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b73493",
   "metadata": {},
   "source": [
    "Dies können wir entweder im MLFlow UI machen, via dem Button `Register model` in der Run Ansicht. Oder wir können es direkt aus dem `log_model()` Aufruf machen, mittels dem Parameter `registered_model_name=\"some model name\"`. Oder wir tun es mit der Funktion [mlflow.register_model()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.register_model) oder via API (siehe unten)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3c66d",
   "metadata": {},
   "source": [
    "Wählen wir einen Modell-Namen, welcher bisher noch nicht existiert, wird ein neues Modell in Version 1.0 registriert. Wenn wir einen Namen wählen, welcher bereits existiert, wird das Modell unter diesem Namen um eine Version höher registriert. Probiere dies einmal aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed228ac",
   "metadata": {},
   "source": [
    "Registrierte Modelle erscheinen unter dem `Models` Tab. Ein Modell hat einen Namen, eine Version und Tags. Versionen können einen Alias haben, diese erklären wir gleich weiter unten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8456bd",
   "metadata": {},
   "source": [
    "## MLFlow Client API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b5ef8",
   "metadata": {},
   "source": [
    "Bisher haben wir mit dem higher level mlflow Modul gearbeitet. Es existiert aber noch ein zweites API, welches low level Zugriff auf MLFlow erlaubt, der [MLFlow Client](https://mlflow.org/docs/latest/python_api/mlflow.client.html), welcher ein simples CRUD Interface für Experimente, Runs und Models zur Verfügung stellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632dded3",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Verwende den MLFlow Client, um alle registrierten Modelle aufzulisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6326bb",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.search_registered_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada66ec",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df4234",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Hole ein Modellmittels `get_model_version()` und gib dessen Description, Alias und Version aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763829a0",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "model = client.get_model_version(name='name of one of your models', version=\"1\")\n",
    "model.description, model.aliases, model.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd18dfe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef03332",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Füge deinem Modell mittels `update_model_version()` eine description hinzu. Die description hängt an der Version. Es gibt auch eine description, welche am Modell hängt. Prüfe danach im UI, ob dies geklappt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd69d37",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "client.update_model_version(name='name of one of your models', version=\"1\", description=\"foobar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb1084",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335d0f7",
   "metadata": {},
   "source": [
    "## Modell Versions-Aliase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722d370",
   "metadata": {},
   "source": [
    "Modelle können entweder via Pfad, via Name+Version oder via Alias geladen werden, vergleiche hierzu die [Dokumentation](https://mlflow.org/docs/latest/getting-started/registering-first-model/step3-load-model.html?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd25c93",
   "metadata": {},
   "source": [
    "Modell Versions-Aliase sind Identifier, welche an einer Version hängen. Sie werden dazu verwendet, den aufrufenden Code von der Modell-Version zu entkoppeln. Wir können unserem aktiven, deployten Modell den Alias *champion* geben und es via diesen referenzieren. Entwickeln wir ein neues, verbessertes Modell, können wir diesem den Alias *champion* zuweisen und müssen den aufrufenden Code nicht anpassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df92c6",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Registeriere dein bestes Modell, wenn noch nicht geschenhen, dann gib ihm den Alias 'champion' mittels der Funktion `set_registered_model_alias()`. Du wirst dieses Modell später noch verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841fcef6",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(name='name of one of your models', version=\"1\", alias='champion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9365eb6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d489d0",
   "metadata": {},
   "source": [
    "**Bitte quittiere wiederum auf [Mentimeter](https://www.menti.com/alaxbnek73eu), dass du mit der Übung durch bist**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
