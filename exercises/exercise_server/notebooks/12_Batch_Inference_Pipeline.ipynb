{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6967c068",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Achtung:</b> Der hier angezeigt Code kann nicht direkt im Exercise Server ausgeführt werden. Der Code fuktioniert nur im <a href=\"http://localhost:8080/lab\">Jupyter Lab der Development Environment</a>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6576270",
   "metadata": {},
   "source": [
    "# Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f577f7",
   "metadata": {},
   "source": [
    "Unser Modell ist fertig trainiert und in der Registry abgelegt. Nun möchten wir es benutzen, um Vorhersagen mittels einer Batch Pipeline zu machen. Frische (Roh-)Daten, für welche wir eine Prediction benötigen, werden einem Umsystem bzw. von einem externen Prozess entweder in Batches oder laufend in den Object Store geschrieben. Unsere Pipeline wird periodisch, zum Beispiel einmal pro Nacht, aktiviert, holt die Daten der vergangenen Periode, macht Predictions für diese Daten und schreibt das Resultat zurück in den Object Store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86aef3",
   "metadata": {},
   "source": [
    "# Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee76b4",
   "metadata": {},
   "source": [
    "Erstelle ein neues Jupyter Notebook für diese Übung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cb455",
   "metadata": {},
   "source": [
    "# Übungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062a73d",
   "metadata": {},
   "source": [
    "## Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba461084",
   "metadata": {},
   "source": [
    "Wir fügen unserer Architektur die erste Pipeline hinzu. Die Pipeline holt das Modell aus MLFlow. Die Daten für die Inferenz liest sie, zum Beispiel einmal jede Nacht, aus dem Object Store. Dorthin schreibt sie auch die Predictions zurück."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27871900",
   "metadata": {},
   "source": [
    "![batch_pipeline_01.png](batch_pipeline_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb4258",
   "metadata": {},
   "source": [
    "## Inferenz-Daten simulieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b643c",
   "metadata": {},
   "source": [
    "Zuerst benötigen wir Code, um einen Bucket mit Daten, für welche wir Inferenz machen wollen, zu erstellen.\n",
    "\n",
    "Wir machen es uns hier einfach und stellen keine frischen Daten her, sondern verwenden die Trainingsdaten. In der Übung zu Stream Processing werden wir dann frische Daten auf eine bessere Weise simulieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1f337",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Lade die Trainingsdaten, entferne das Label und füge als erste Column im DataFrame eine Spalte mit Namen `event_date` hinzu. Das Event Date soll vom Typ `datetime64[ns]` sein und je ca. ein Viertel der Rows soll einen der folgenden vier Werte haben: `2024-09-11`, `2024-09-12`, `2024-09-13`, `2024-09-14`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d34cb0",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet('s3://traindata/train_raw.parquet', storage_options={\"anon\": False}).drop('class', axis='columns')\n",
    "\n",
    "# quick and dirty way to add an event date column\n",
    "df.insert(0, 'event_date', None)\n",
    "num_days = 4\n",
    "for chunk, date_ in zip(np.array_split(df, num_days), pd.date_range(\"2024-09-11\", periods=num_days, freq=\"d\")):\n",
    "    chunk.event_date = date_\n",
    "    df = pd.concat([df, chunk], axis='rows')\n",
    "    \n",
    "# we need to remove all original rows, those are now duplicates that do not have a value in the event_date column\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00008be9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12aeda",
   "metadata": {},
   "source": [
    "Wir erstellen einen neuen Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f00b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "s3.mkdir(\"inferencedata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774f420",
   "metadata": {},
   "source": [
    "Und legen die Daten ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('s3://inferencedata/inference_raw.parquet', storage_options={\"anon\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d6177",
   "metadata": {},
   "source": [
    "## Modell laden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d7e62",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Nun beginnt der Code der Pipeline.\n",
    "\n",
    "Lade als erstes dein Modell aus MLFlow. Du kannst das Modell auf verschiedene Arten referenzieren. Im Idealfall machst Du dies via Modell-Namen und einen Alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b4493",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# we never defined an official name, make sure you use the name of your own registered model here\n",
    "model_name = \"mushroom\"\n",
    "\n",
    "# this also only works if you set this alias for above model\n",
    "model_version_alias = \"champion\"\n",
    "\n",
    "model_uri = f\"models:/{model_name}@{model_version_alias}\"\n",
    "model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5f037",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12b3b5",
   "metadata": {},
   "source": [
    "## Daten laden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f8b9a",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Als nächstes lädst du die gerade eben generierten Daten. Lade aber nur die Daten von *gestern*, und nicht alle Daten. Damit ist nicht gemeint, dass du alle Daten lädst und dann filterst, sondern filtere bereits beim Laden nach dem Datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8976375",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "yesterday = pd.Timestamp.today() - pd.Timedelta(days=1)\n",
    "\n",
    "# only for debugging purposes\n",
    "# yesterday = pd.to_datetime('2024-09-13') - pd.Timedelta(days=1)\n",
    "\n",
    "sel = [(\"event_date\", \"==\", yesterday)]\n",
    "df = pd.read_parquet('s3://inferencedata/inference_raw.parquet', filters=sel, storage_options={\"anon\": False})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5c186",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29634156",
   "metadata": {},
   "source": [
    "## Inferenz duchführen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44ee90",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Führe nun die Inferenz auf den geladenen Daten durch. Hänge eine Spalte mit Namen `prediction` hinten an den DataFrame `df` an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc261f48",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "df['prediction'] = model.predict(df.drop('event_date', axis='columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68481508",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f983c",
   "metadata": {},
   "source": [
    "## Resultat zurück in den Object Store schreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677764ab",
   "metadata": {},
   "source": [
    "Nun kannst Du das resultat zurückschreiben. Wir sind bequem und verzichten darauf, einen neuen Bucket zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('s3://inferencedata/inference_prediction.parquet', storage_options={\"anon\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0180e",
   "metadata": {},
   "source": [
    "## Wrapup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e893f",
   "metadata": {},
   "source": [
    "Du hast in dieser Übung eine einfache Batch Processing Pipeline mit Python/Pandas gebaut, welche Inferenz für unser Mushroom Modell durchführt und das Resultat zurück in den Object Store schreibt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf092e",
   "metadata": {},
   "source": [
    "Dabei haben wir auch ein paar Abkürzungen genommen, was im Rahmen eines Kurses zwar vertretbar ist, deren wir uns daber auch bewusst sein sollten:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae2ecc",
   "metadata": {},
   "source": [
    "* Batch Processing wird normalerweise mit Spark / PySpark gemacht und nicht mit Pandas. Der Unterschied ist hier aber nicht relevant.\n",
    "* Wir arbeiten mit einfachen Timestamps. Kein Handling von Zeitzonen, keine Konvertierung in UTC. Normalisierung von Timestamps nach UTC ist ein wichtiger Schritt in Datenpipelines.\n",
    "* Wir loggen nichts, insb. keine operativen Metriken wie Speicherverbrauchb\n",
    "* Keine Tests, kein sauberes Error Handling\n",
    "* Unsere Pipeline lebt in einbem Jupyter Notebook. Um sie korrekt zu operationalisieren, sollte sie in Form eines Skriptes von einem Orchestrierungstool wie Apache Airflow aufgerufen werden. Dazu später mehr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38664a",
   "metadata": {},
   "source": [
    "Und insbesondere:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a35c3f",
   "metadata": {},
   "source": [
    "Wir arbeiten mit den Rohdaten und machen kein Feature Engineering. Auch wenn wir ohne weiteres in unserer Batch Pipeline Features berechnen könnten, wird dies, sobald die Anzahl von Features und Modellen zunimmt, unhandlich.\n",
    "\n",
    "Die Lösung ist die verwendung eines Feature Stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef703d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966093f",
   "metadata": {},
   "source": [
    "**Bitte quittiere wiederum auf [Mentimeter](https://www.menti.com/alaxbnek73eu), dass du mit der Übung durch bist**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
