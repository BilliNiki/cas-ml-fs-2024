{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6967c068",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Achtung:</b> Der hier angezeigt Code kann nicht direkt im Exercise Server ausgeführt werden. Der Code fuktioniert nur im <a href=\"http://localhost:8080/lab\">Jupyter Lab der Development Environment</a>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6576270",
   "metadata": {},
   "source": [
    "# Streaming Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f577f7",
   "metadata": {},
   "source": [
    "Nachdem wir eine Pipeline für Batch Inference gebaut haben, wollen wir nun Predictions in Echtzeit, aus einer Stream Processing Pipeline heraus, machen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86aef3",
   "metadata": {},
   "source": [
    "# Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee76b4",
   "metadata": {},
   "source": [
    "Erstelle ein neues Jupyter Notebook für diese Übung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cb455",
   "metadata": {},
   "source": [
    "# Übungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062a73d",
   "metadata": {},
   "source": [
    "## Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba461084",
   "metadata": {},
   "source": [
    "Wir fügen unserer Architektur eine weitere Pipeline hinzu. Die Pipeline holt wiederum das Modell aus MLFlow. Die Daten für die Inferenz erhält sie jedoch von Kafka, und dorthin schreibt sie auch die Predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27871900",
   "metadata": {},
   "source": [
    "![streaming_pipeline_01.png](streaming_pipeline_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d6177",
   "metadata": {},
   "source": [
    "## Modell laden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d7e62",
   "metadata": {},
   "source": [
    "Dieser Teil bleibt gleich wie bei der Batch Pipeline. Importiere MLFlow, lade unser Modell via Champion Alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# we never defined an official name, make sure you use the name of your own registered model here\n",
    "model_name = \"mushroom\"\n",
    "\n",
    "# this also only works if you set this alias for above model\n",
    "model_version_alias = \"champion\"\n",
    "\n",
    "model_uri = f\"models:/{model_name}@{model_version_alias}\"\n",
    "model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12b3b5",
   "metadata": {},
   "source": [
    "## Quix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c710dd",
   "metadata": {},
   "source": [
    "Bisher haben wir via das Python Modul [kafka-python](https://kafka-python.readthedocs.io/en/master/) mit Kafka kommuniziert. Unsere Pipeline werden wir jedoch mit [Quix Streams](https://quix.io/) schreiben. Quix wirbt mit folgendem Slogan für ihr Produkt:\n",
    "\n",
    "> Python stream processing made simple\n",
    "\n",
    "> Pure Python. No JVM. No wrappers. No cross-language debugging. Use Streaming DataFrames and the whole Python ecosystem to develop stream processing pipelines in fewer lines of code.\n",
    "\n",
    "Quix erlaubt auf relativ einfache Weise, in Python Stream Processing Pipelines zu schreiben, und dabei durch Kafka- und Kubernetes-Unterstützung sehr skalierbar zu bleiben. Insbesondere wird keine komplexe Basisinfrastruktur benötigt, um loslegen zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a64866",
   "metadata": {},
   "source": [
    "Lies dir zum Einstieg die drei [Core Concepts](https://quix.io/docs/get-started/welcome.html) durch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0251a",
   "metadata": {},
   "source": [
    "Wird werden mit einem StreamingDataFrame lokal arbeiten, d.h. die erwähnte Quix Cloud verwenden wir nicht. Die Dokumentation zum StreamingDataFrame findest du [hier](StreamingDataFrame). *Step 3* des [Quickstarts](https://quix.io/docs/quix-streams/quickstart.html) zeigt ein Beispiel, wie wir vorgehen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b128ed",
   "metadata": {},
   "source": [
    "## Quix Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd101f",
   "metadata": {},
   "source": [
    "Initialisiere Quix Streams, analog dem Beispiel aus dem Quickstart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams import Application\n",
    "\n",
    "# create main quix object\n",
    "app = Application(\n",
    "    broker_address=\"message-broker:9092\",\n",
    ")\n",
    "\n",
    "# define topic and message format\n",
    "input_topic = app.topic(name=\"mushroom_inference_request\", value_deserializer=\"json\")\n",
    "\n",
    "# create a StreamingDataFrame\n",
    "sdf = app.dataframe(topic=input_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703885a0",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Nun hast du einen Streaming DataFrame, welcher aus Kafka Event notifications des Topics *mushroom_inference_request* erhält. Erweitere den obigen Code um eine Zeile, welche jede erhaltene Message ausgibt und um eine weitere Zeile, welche den ganzen Code dann ausführt.\n",
    "\n",
    "**Achtung: Obigen Initialisierungs-Code musst du jedesmal, wenn du den Consumer gestoppt hast, erneut ausführen, damit Quix wieder zu konsumieren beginnt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb79edf",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Die `StreamingDataFrame.update()` Funktion erlaubt es, Side Effects auszuführen. Wir verwenden sie, um als Side Effect bei jedem Update des DataFrames, also bei jeder eingehenden Message, den Inhalt dieser message auszugeben. Unsere messages kommen als dict daher und `print` gibt dann einfach diesen `dict` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fab81",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "sdf = sdf.update(print)\n",
    "\n",
    "# the following is the same but (unnecessarily) more verbose\n",
    "# sdf = sdf.update(lambda message: print(f\"Inference Request: {message}\"))\n",
    "\n",
    "# and this line starts the consumer\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581e918",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df9524",
   "metadata": {},
   "source": [
    "Wenn du Deinen Code ausführst, solltest du Log Output sehen, welcher das Setup zeigt und meldet, dass auf eingehende Nachrichten gewartet wird. Diese musst du nun produzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8861c4",
   "metadata": {},
   "source": [
    "Starte den Datengenerator wie folgt und brich ihn dann nach ein, zwei Meldungen wieder ab (ctrl-c). Als Erinnerung: Damit das funkioniert, musst du dich in oder unterhalb dem Ordner befinden, in welchem das `docker-compose.yml` mit allen Servcices liegt.\n",
    "\n",
    "````\n",
    "docker compose run --rm --name=datagen --entrypoint=python3 development_env mushroom_datagen.py -v\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37435c7",
   "metadata": {},
   "source": [
    "Du solltest die von Datengenerator gesendeten Inference Requests sehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1b54f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f789fef",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Nun soll die Nachricht nicht mehr ausgegeben werden, sondern das Modell aufgerufen und die Prediction als weitere Spalte angehängt werden.\n",
    "\n",
    "Dazu muss:\n",
    " * Die message von einem dict in einen Pandas DataFrame (oder ein 2D Numpy array) konvertiert werden\n",
    " * Die Prediction gemacht werden\n",
    " * Das Resultat der Prediction in einen Integer Scalar konvertiert werden\n",
    " * Dieser Integer mit dem Key 'prediction' an die ursprüngliche message (dict) angehängt werden\n",
    " \n",
    "Versuche, dies mit sdf.update() analog oben zu erreichen. Gib zur Kontrolle danach (auch analog oben) das Resultat aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce22d9b",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# als Zweizeiler, etwas kryptisch:\n",
    "sdf = sdf.update(lambda m: m.update({'prediction': int(model.predict(pd.DataFrame(m, index=[0]))[0])}))\n",
    "sdf = sdf.update(print)\n",
    "\n",
    "\n",
    "# oder mit separater Funktion, Schritt für Schritt\n",
    "\n",
    "def predict(d: dict) -> int:\n",
    "    # reads single row dict and returns prediction as integer scalar\n",
    "    # spelled out to make it easier to read\n",
    "    \n",
    "    input_as_df = pd.DataFrame(d, index=[0])\n",
    "    pred_as_array = model.predict(input_as_df)\n",
    "    pred_as_scalar = pred_as_array[0]\n",
    "    \n",
    "    return int(pred_as_scalar)\n",
    "\n",
    "# das innere update ist die update Funktion des python dictionaries und hat nichts mit quix zu tun\n",
    "sdf.update(lambda m: m.update({'prediction': predict(m)}))\n",
    "sdf = sdf.update(print)\n",
    "\n",
    "\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f6338",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d92b7b",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Schliesslich wollen wir unter einem neuen topic unsere prediction ausgeben. Definiere dazu ein topic `mushroom_inference_result` und bringe den StreamingDataFrame dazu, jedes Resultat an dieses topic zu schicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e2f50",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "output_topic = app.topic(name=\"mushroom_inference_result\", value_deserializer=\"json\")\n",
    "\n",
    "sdf.to_topic(output_topic)\n",
    "\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d985440",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f983c",
   "metadata": {},
   "source": [
    "Sende zum Schluss einige Messages (inference requests) und prüfe in Redpanda, ob die dazugehörigen prediction results als messages ankommen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0180e",
   "metadata": {},
   "source": [
    "## Wrapup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e893f",
   "metadata": {},
   "source": [
    "Du hast in dieser Übung eine Stream Processing Pipeline gebaut, welche Inferenz-Requests für unser Mushroom Modell entgegennimmt, mit unserem Modell eine Prediction macht und das Resultat zurück an Kafka sendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf092e",
   "metadata": {},
   "source": [
    "Dabei haben wir auch ein paar Abkürzungen genommen, was im Rahmen eines Kurses zwar vertretbar ist, deren wir uns daber auch bewusst sein sollten:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae2ecc",
   "metadata": {},
   "source": [
    "* Wir haben uns nicht um Skalierbarkeit gekümmert. Mit unserem Aufbau und Quix ist die Pipeline einfach skalierbar\n",
    "* Unser Meldungsformat war sehr einfach und ohne jegliche Metadaten\n",
    "* Unser Meldungsformat war ein einfaches json, ohne Schema und Validierung\n",
    "* Einige Dinge haben wir hartkodiert bzw. nicht sauber entkoppelt (z.B. Feature Typen, Topic)\n",
    "* Keine Tests, kein sauberes Error Handling, kein Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051238b4",
   "metadata": {},
   "source": [
    "Und inbesondere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0bc46",
   "metadata": {},
   "source": [
    "Wir machen Inferenz mit Realtime Daten, welche alle im Request daher kommen. Wir verwenden also keine vorausberechneten Features, wir joinen keine anderen Feature-Quellen. Wer hier mehr wissen möchte, muss sich vertieft mit Stream Processing auseinandersetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888bdce",
   "metadata": {},
   "source": [
    "**Bitte quittiere wiederum auf [Mentimeter](https://www.menti.com/alaxbnek73eu), dass du mit der Übung durch bist**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
