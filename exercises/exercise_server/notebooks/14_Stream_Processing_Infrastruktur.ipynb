{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d7c61c",
   "metadata": {},
   "source": [
    "# Streaming Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6784ff",
   "metadata": {},
   "source": [
    "In dieser Übung setzen wir den Message Broker Dienst auf und lernen diesen kennen. Wird verwenden als Message Broker [Redpanda](https://www.redpanda.com/). Redpanda beschreibt ihr Produkt wie folgt:\n",
    "\n",
    "> Redpanda is the most complete, Apache Kafka®-compatible streaming data platform, designed from the ground up to be lighter, faster, and simpler to operate. Free from ZooKeeper™ and JVMs, it prioritizes an end-to-end developer experience with a huge ecosystem of connectors, configurable tiered storage, and more.\n",
    "\n",
    "Für uns in der Übungsinfrastruktur ist vor allem die einfachere, leichtegwichtigere Installation im Vergleich zum Standard [Apache Kafka](https://kafka.apache.org/) wichtig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9ceb9",
   "metadata": {},
   "source": [
    "# Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020387a7",
   "metadata": {},
   "source": [
    "Speichere alle deine offenen Notebooks. Stoppe alle Container, indem du dich ins Top-Level Verzeichnis der Übungen begibst (dort, wo das File `docker-compose.yml` liegt) und `docker compose down` ausführst. Wenn du die Container im Vordergrund laufen hast, kannst du in diesem Terminal die Prozesse mit `ctrl-c` stoppen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a3d15",
   "metadata": {},
   "source": [
    "Kommentiere im Top-Level Compose File `docker-compose.yml` den Message Broker Dienst ein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb7fa9",
   "metadata": {},
   "source": [
    "Starte alle Container wieder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cc657",
   "metadata": {},
   "source": [
    "# Übungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758b290",
   "metadata": {},
   "source": [
    "## Redpanda Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264c8b4",
   "metadata": {},
   "source": [
    "Wir erweitern unsere Infrastruktur um einen Message Broker, um eine [Event-Driven Architecture](https://en.wikipedia.org/wiki/Event-driven_architecture) zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4931c",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Wir hatten in der ersten Übung die beiden Docker Netzwerke `development` und `production` definiert. In welches dieser Netze binden wir den Message Broker ein?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aacaf3",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Korrekt wäre, Produktion und Entwicklung hier auf eine geeignete Art und Weise streng zu trennen, entweder durch zwei separate Instanzen oder durch anderweitige Massnahmen. Für den Workshop begrenzen wir die Komplexität aber, indem wir den gleichen Message Broker in beiden Docker Netzwerken `development` und `production` verwenden, auch wenn diese Lösung ist nicht ideal ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c51c3e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2184cc",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Schau Dir das compose file des Message Brokers an. Naturgemäss ist für einen Infrastruktur-Dienst, welcher darauf ausgelegt ist, unter Kubernetes hochverfügbar und horizontal skalierbar zu laufen, das compose File etwas komplexer. \n",
    " * Via welchen Port kannst du von ausserhalb Docker auf den Message Broker zugreifen?\n",
    " * Welche zwei APIs werden im File definiert?\n",
    " * Über welche URI nutzen wir den Broker, wenn wir via Kafka (und nicht via HTTP) zugreifen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552ac6e",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* Von ausserhalb Docker kannst du nur über die Redpanda Console zugreifen, via. den Port 8085:  Probiere dies einmal: [http://localhost:8085](http://localhost:8085)\n",
    "* Einerseits das Kafka API, andererseits der Pandaproxy (ein HTTP REST API)\n",
    "* Die Antwort findest du in der Direktive `command`, etwas versteckt:\n",
    "\n",
    "      --advertise-kafka-addr internal://message-broker:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efc8ad",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fef1d1",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Unsere Umgebung sieht nun so aus, wobei der Message Broker korrekterweise nur ins Production Network gehörenb würde. Die Batch-Inference Pipeline ist in untenstehendem Diagramm nicht abgebildet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8db4e5",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "![summary.png](summary_message_broker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf872c",
   "metadata": {},
   "source": [
    "## Topic mit der Console erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a0b91",
   "metadata": {},
   "source": [
    "Wir benötigen nun ein Topic, um darüber Nachrichten verschicken zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e46256",
   "metadata": {},
   "source": [
    "Verwende die [Redpanda Console](http://localhost:8085), um ein Topic mit dem Namen `dev_sandbox` zu erstellen. Belasse sonst alles bei den Defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa364877",
   "metadata": {},
   "source": [
    "## Command Line Tool rpk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6f866",
   "metadata": {},
   "source": [
    "In jedem Redpanda Broker-Container ist das Redpanda CLI Tool `rpk` installiert. Hier findest Du die [Command Reference](https://docs.redpanda.com/current/reference/rpk/) von `rpk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abd529",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Verwende `rpk` direkt im laufenden Message Broker Container, um \"Informationen\" zum laufenden \"Cluster\" anzuzeigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334af0f6",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Du verwendest führst mittels `docker exec` interaktiv (`-it`) im Container `message-broker` den Befehl `rpk cluster info` aus:\n",
    "\n",
    "    docker exec -it message-broker rpk cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c71e6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e3ef7",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Schicke nun mindestens zwei Nachrichten im vorher erstellten Topic (hint: verwende `rpk topic produce`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498877e",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. `docker exec -it message-broker rpk topic produce dev_sandbox`\n",
    "1. Nachricht tippen und mit <enter> abschliessen\n",
    "1. Mit ctrl-c abbrechen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07962a0b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12833765",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Schau in der [Redpanda Console](http://localhost:8085), ob Du die Nachrichten siehst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcfe53",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Unter Topic aufs `dev_sandbox` Topic klicken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639b30b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298c4d3",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Nun konsumiere eine einzelne Nachricht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf5b04",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "`docker exec -it message-broker rpk topic consume dev_sandbox --num 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3522aa",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fad3f",
   "metadata": {},
   "source": [
    "Wenn du auf die gleiche Weise noch eine Nachricht konsumierst, erhälst du nocheinmal die erste Nachricht. Kafka bzw. Redpanda trackt nicht, wer welche Nachrichten konsumiert, und Nachrichten können nicht selektiv gelöscht werden. Diese Implementation als append-only log ist einer der Gründe, weshalb die Architektur so gut horizontal skaliert.\n",
    "\n",
    "Messages haben einen [consumer offset](https://docs.redpanda.com/current/develop/consume-data/consumer-offsets/), welcher verwendet wird, um Nachrichten ab einem spezifischen Punkt zu lesen. Es liegt in der Verantwortung des Consumers, diesen Offset zu tracken oder Kafka mitzuteilen, bis zu welchem Offset gelesen wurde.\n",
    "\n",
    "High-Level APIs machen dies automatisch, wenn wir ein Low-Level API wie rpk verwenden, müssen wir dies selber tun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f6180",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Schau in der [Redpanda Console](http://localhost:8085) nach, welchen Offset die zweite Nachricht hat, und lies genau diese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406e724",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Du gibst den zu lesenden Offset mit `--offset` an:\n",
    "\n",
    "`docker exec -it message-broker rpk topic consume dev_sandbox --offset 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b0d95",
   "metadata": {},
   "source": [
    "## Consumer Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719dd86",
   "metadata": {},
   "source": [
    "Ein Konzept, welches wir noch grob kennen müssen, ist das der `Consumer Groups`. Über diese können sich Consumer in Gruppen einteilen. Warum Consumer Groups notwendig sind, kannst Du [hier](https://www.redpanda.com/guides/kafka-architecture-kafka-consumer-group) nachlesen.\n",
    "\n",
    "Für uns wichtig ist, dass eine message von genau einem Consumer je Consumer Group gelesen werden kann. Lesen wir eine message als Teil einer Consumer Group, können wir (automatisch) den offset der message committen, so dass diese message danach nicht ein zweites mal gelesen wird. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38173d",
   "metadata": {},
   "source": [
    "## Konsumieren mit kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819e580",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Achtung:</b> Der hier angezeigt Code kann nicht direkt im Exercise Server ausgeführt werden. Der Code fuktioniert nur im <a href=\"http://localhost:8080/lab\">Jupyter Lab der Development Environment</a>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea473506",
   "metadata": {},
   "source": [
    "Nun verwenden wir die library [kafka-python](https://kafka-python.readthedocs.io/en/master/), um mit Nachrichten zu konsumieren. Kafka-python ist einfacher zu verwenden als die library [confluent-kafka-python](https://github.com/confluentinc/confluent-kafka-python), hat jedoch auch etwas weniger Features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0882a42",
   "metadata": {},
   "source": [
    "Erstelle ein neues Notebook, um mit dem folgenden Code zu spielen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac6f66",
   "metadata": {},
   "source": [
    "Der folgende Code importiert den Kafka Consumer und instanziert den Consumer mit einer Consumer Group ID. Dadurch wird beim Lesen der consumer offset einer message automatisch committet.\n",
    "\n",
    "Soll nicht automatisch committet werden, übergib auch die auskommentierten Argumente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13914a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers=[\"message-broker:9092\"],\n",
    "    group_id=\"sandbox-group\",\n",
    "    # auto_offset_reset=\"earliest\",\n",
    "    # enable_auto_commit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398f1b0",
   "metadata": {},
   "source": [
    "Abonniere unser Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.subscribe(\"dev_sandbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944f19f",
   "metadata": {},
   "source": [
    "Und der folgende Code liest kontinuierlich und gibt jede gelesene message mit einigen Metadaten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for message in consumer:\n",
    "        topic_info = f\"topic: {message.partition}|{message.offset}\"\n",
    "        message_info = f\"key: {message.key}, {message.value}\"\n",
    "        print(f\"{topic_info}, {message_info}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while consuming messages: {e}\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731621d8",
   "metadata": {},
   "source": [
    "Verwende `rpk`, um einige messages zu schicken und betrachte, wie sie gelesen werden. Experimentiere mit den auskommentierten Argumenten, um ein Gefühl dafür zu bekommen, wie die Offsets funktionieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef68897",
   "metadata": {},
   "source": [
    "## Produzieren mit kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c09c3b",
   "metadata": {},
   "source": [
    "Als nächstes schreiben wir asynchron messages. Wiederum importieren wir, instanziieren danach einen producer und setzen unser topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = \"message-broker:9092\"\n",
    ")\n",
    "\n",
    "topic = \"dev_sandbox\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d512a",
   "metadata": {},
   "source": [
    "Nun definieren wir zwei Callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb928a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_success(metadata):\n",
    "    print(f\"Message produced to topic '{metadata.topic}' at offset {metadata.offset}\")\n",
    "\n",
    "def on_error(e):\n",
    "    print(f\"Error sending message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c5fd5",
   "metadata": {},
   "source": [
    "Schliesslich produzieren wir drei Nachrichten asynchron und flushen und schliessen den producer wieder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20551118",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    msg = f\"Message with id #{i}\"\n",
    "    future = producer.send(topic, value=str.encode(msg))\n",
    "    future.add_callback(on_success)\n",
    "    future.add_errback(on_error)\n",
    "    \n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833e149",
   "metadata": {},
   "source": [
    "## Generator für Mushroom Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29abdf97",
   "metadata": {},
   "source": [
    "Kehren wir zurück zu unserem Datenset über Pilze, mit welchem wir ein Modell trainiert und bereits eine Batch Inference Pipeline geschrieben haben. Wir möchten nun anstelle einer Batch Prediction im Rahmen unserer neu aufgesetzten Event-getriebenen Architektur in realtime Predictions machen. Dazu brauchen wir eine Quelle, welche unsere produktiven Inferenz-Requests simuliert. Die Quelle soll kontinuierlich Pilzdaten generieren und als messages im Sinne eines *Request for Prediction* nach Redpanda schreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cc0c9",
   "metadata": {},
   "source": [
    "Das Endprodukt soll ein Python Skript sein, welches über die Kommandozeile aufgerufen werden kann. Das Skript soll `mushroom_datagen.py` heissen und im `/notebooks`  Verzeichnis des Development Server liegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8467fe1",
   "metadata": {},
   "source": [
    "Das Skript entwickeln kannst Du entweder innerhalb eines Jupyter Notebooks, oder direkt als Skript mit VS Code, wie Du möchtest.\n",
    "\n",
    "Wenn Du das Skript von ausserhalb der Container-Umgebung ausführen willst, kannst Du dies mit dem folgenden Befehl machen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba91fa",
   "metadata": {},
   "source": [
    "```\n",
    "docker compose run --rm --name=datagen --entrypoint=python3 development_env mushroom_datagen.py\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad67461",
   "metadata": {},
   "source": [
    "Num zum zu entwickelnden Skript. Du bekommst den Rahmen vorgegeben und entwickelst dann einzelne Funktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dd839",
   "metadata": {},
   "source": [
    "Verwende die folgenden Imports. Du kannst natürlich auch weitere Imports hinzufügen, wenn dein Code welche braucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import loguru\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d3f25",
   "metadata": {},
   "source": [
    "Nun schreibe die notwendigen Funktionen, um nach Kafka zu senden:\n",
    " * initialisierte den Kafka Producer (ohne Serializer)\n",
    " * schreibe die Success- und Error Callbacks. Die Callbacks sollen nur eine entsprechende Debug- oder Error Meldung loggen und haben sonst keine weitere Aufgabe. \n",
    " * schreibe die Funktion, welche eine Message an Kafka sendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113b432",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "    # your code here\n",
    ")\n",
    "\n",
    "\n",
    "def on_success(metadata):\n",
    "    logger.debug()  # your code inside the parenthesis\n",
    "\n",
    "    \n",
    "def on_error(e):\n",
    "    logger.error()  # your code inside the parenthesis\n",
    "    \n",
    "    \n",
    "def push_to_kafka(event, topic):\n",
    "    # your code here\n",
    "    # send an utf-8 encoded message with key=b'key'\n",
    "    # add callbacks\n",
    "    # flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f685e78",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# everything quite similar to the examples above (except for the utf-8 encoding)\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = \"message-broker:9092\",\n",
    ")\n",
    "\n",
    "\n",
    "def on_success(metadata):\n",
    "    logger.debug(f\"Message produced to topic '{metadata.topic}' at offset {metadata.offset}\")\n",
    "\n",
    "\n",
    "def on_error(e):\n",
    "    logger.error(f\"Error sending message: {e}\")\n",
    "\n",
    "\n",
    "def push_to_kafka(event, topic):\n",
    "    future = producer.send(topic=topic, key=b'key', value=event.encode('utf-8')) \n",
    "    future.add_callback(on_success)\n",
    "    future.add_errback(on_error)\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21340c41",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74588c6",
   "metadata": {},
   "source": [
    "Als Nächstes schreibe eine Funktion, welche jedesmal, wenn sie aufgerufen wird, einen einzelnen Inferenz-Request als json zurückgibt, in der folgenden Form:\n",
    "\n",
    "````\n",
    "{\n",
    "   \"cap-diameter\":1382.0,\n",
    "   \"cap-shape\":5.0,\n",
    "   \"gill-attachment\":3.0,\n",
    "   \"gill-color\":4.0,\n",
    "   \"stem-height\":0.7349878012,\n",
    "   \"stem-width\":1583.0,\n",
    "   \"stem-color\":11.0,\n",
    "   \"season\":1.4574740759\n",
    "}\n",
    "````\n",
    "\n",
    "Die Werte-Bereiche sollten in etwa denen aus dem Trainingsdaten-file entsprechen, sie dürfen also nicht komplett zufällig sein. Insbesondere sollten keine kategorischen Werte produziert werden, die ausserhalb des Intervalls `[min(col), max(col)]` ein jeder Spalte liegen, und keine negativen Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb15a1",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def generate_event(# whatever you need):\n",
    "\n",
    "    # your code here\n",
    "\n",
    "    return # somehing as json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ecd99",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Hier eine mögliche Umsetzung mittels zweier Funktionen.\n",
    "\n",
    "Zuerst werden die Trainingsdaten aus dem Object Store geladen. Dann wird ein Kernel Density Estimator auf die numerischen Spalten gefittet, um eine einfache Möglichkeit zu haben, zufällige, aber ähnliche Werte zu generieren.\n",
    "\n",
    "Um den Output zu generieren, sampeln wir die numerischen Spalten. Die kategorischen ziehen wir zufällig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b83ef1",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def setup_data():\n",
    "    # fetch Training Dataset so we have a reference\n",
    "    df = pd.read_parquet('s3://traindata/train_raw.parquet',\n",
    "                         storage_options={\"anon\": False}).drop(\"class\", axis=\"columns\")\n",
    "\n",
    "    # setup column names\n",
    "    categoricals = ['cap-shape', 'gill-attachment', 'gill-color', 'stem-color']\n",
    "    numericals = [c for c in df.columns if c not in categoricals]\n",
    "\n",
    "    # fit an estimator to the numerical columns\n",
    "    kde = KernelDensity()\n",
    "    kde.fit(df[numericals])\n",
    "\n",
    "    return kde, numericals, categoricals, df\n",
    "\n",
    "\n",
    "def generate_event(kde, numericals, categoricals, df):\n",
    "    # take one row so we have something to fill in our generated values\n",
    "    new_row = pd.DataFrame(data=df.head(1))\n",
    "\n",
    "    # generate one row\n",
    "    new_row[numericals] = kde.sample(1)\n",
    "    for col in numericals:\n",
    "        # we are being lazy, when the kde yields a negative, just replace with mean\n",
    "        if new_row[col][0] < 0:\n",
    "            new_row[col] = df[col].mean()\n",
    "    for col in categoricals:\n",
    "        # for the categoricals, use a bounded random value\n",
    "        new_row[col] = randrange(df[col].min(), df[col].max()+1)\n",
    "    for col in new_row.columns:\n",
    "        # make sure the datatypes match the reference\n",
    "        new_row[col] = new_row[col].astype(df[col].dtype)\n",
    "\n",
    "    return new_row.iloc[0].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e32a82",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b4f13",
   "metadata": {},
   "source": [
    "Schliesslich musst Du das ganze in ein Kommandozeilenskript verpacken. Das folgende Snippets legt die notwendigen Argumente fest und ruft die Hauptfunktion `run()` auf (welche du gleich schreiben wirst):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Simulates inference requests by sending mushroom dataset features to kafka.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-t\",\n",
    "        \"--topic\",\n",
    "        type=str,\n",
    "        help=\"Kafka topic to send to\",\n",
    "        default=\"mushroom_inference_request\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--sleep_interval\",\n",
    "        type=int,\n",
    "        help=\"Number of milliseconds to sleep between bursts\",\n",
    "        default=1000,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\",\n",
    "        \"--burst_size\",\n",
    "        type=int,\n",
    "        help=\"Number of messages to send together\",\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-r\",\n",
    "        \"--randomness\",\n",
    "        type=int,\n",
    "        help=\"Maximum number of seconds to wait between bursts\",\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-v\",\n",
    "        \"--verbose\",\n",
    "        action='store_true',\n",
    "        help=\"Log content of each message sent sto stderr\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "    if not args.verbose:\n",
    "        # increase loglevel from the default DEBUG to INFO to avoid logging every message\n",
    "        logger.remove(0)\n",
    "        logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "    logger.info(\"Start sending messages to kafka.\")\n",
    "    run(\n",
    "        topic = args.topic,\n",
    "        sleep_interval = args.sleep_interval,\n",
    "        burst_size = args.burst_size,\n",
    "        randomness = args.randomness\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d7fdd",
   "metadata": {},
   "source": [
    "Die `run()` Funktion soll in einem Endlos-Loop Bursts von Messages an Kafka senden, und nach jedem Burst die definierte Zeit schlafen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77f3eb",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def run(topic, sleep_interval, burst_size, randomness):\n",
    "    # maybe you need code here\n",
    "    while True:\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea6499",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Ein Lösungsansatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6ac6c",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def run(topic, sleep_interval, burst_size, randomness):\n",
    "    kde, numericals, categoricals, df = setup_data()\n",
    "    while True:\n",
    "        for _ in range(burst_size):\n",
    "            push_to_kafka(event=generate_event(kde, numericals, categoricals, df), topic=topic)\n",
    "\n",
    "        try:\n",
    "            sleep_milliseconds = sleep_interval + randrange(0, randomness*1000)\n",
    "        except ValueError:\n",
    "            # if randomness*1000 < 1\n",
    "            sleep_milliseconds = sleep_interval\n",
    "        time.sleep(sleep_milliseconds / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9aafc9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14651e7f",
   "metadata": {},
   "source": [
    "Fertig! Nun testen wir das Skript. Wenn du es wie oben beschrieben abgelegt hast, starte es mittels\n",
    "\n",
    "```\n",
    "docker compose run --rm --name=datagen --entrypoint=python3 development_env mushroom_datagen.py -v\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d7077",
   "metadata": {},
   "source": [
    "Wenn alles klappt,\n",
    " * solltest du nun in der Konsole sehen, wie messages produziert und gesendet werden,\n",
    " * in der [Redpanda Konsole](http://localhost:8085/) kannst du unter dem Topic `mushroom_inference_request` sehen, wie die Meldungen ankommen,\n",
    " * mit deinem Notebook mit dem Consumer Code aus dem Anfang dieser Übung kannst du die Nachrichten lesen\n",
    " \n",
    "**Stoppe nun den Generator wieder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca9577",
   "metadata": {},
   "source": [
    "**Bitte quittiere wiederum auf [Mentimeter](https://www.menti.com/alaxbnek73eu), dass du mit der Übung durch bist**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
